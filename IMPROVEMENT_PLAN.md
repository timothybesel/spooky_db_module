# SpookyDb Module — Improvement Plan

> Generated by spooky-db-analyze-team.
> Agents: DB Architecture · Performance · Clean Code · Systems Correctness

---

## Executive Summary

The `spooky_db_module` persistence layer is well-architected at the serialization and record layers (Layers 1–3) and compiles cleanly with 125 passing tests. The db layer (Layer 4) has **four correctness bugs that can produce persistent data corruption or phantom records without any error surfacing**, the most critical of which is that in-memory ZSets are mutated before redb transactions commit — a failed commit leaves the process in an inconsistent state for its entire remaining lifetime. The `DbBackend` trait also has an object-safety violation that makes the primary SSP integration pattern (`Box<dyn DbBackend>`) impossible to compile, and a silent error-swallowing signature on `get_record_bytes` that converts disk failures into missing-record results. The performance layer is strong for serialization/record reads but the db I/O path has a heap allocation on every single read and write (`make_key`), a missing ZSet guard on `get_version`, and zero benchmark coverage for the entire persistence layer. The three most important fixes in order: **(1) move ZSet mutations to after commit in `apply_mutation`/`apply_batch`**, **(2) enforce the table-name colon invariant with `SpookyDbError::InvalidKey`**, and **(3) fix `DbBackend::bulk_load` to accept `Vec<BulkRecord>` to restore object safety**.

---

## Critical (Fix Before Anything Else)

Issues that risk data correctness or silent corruption. Fix these before any other work.

---

### CRIT-1 — ZSet/Disk Atomicity: ZSet mutated before redb commit, no rollback

**File**: `src/db/db.rs:121–147` (`apply_mutation`), `src/db/db.rs:185–221` (`apply_batch`)
**Confirmed by**: Systems Correctness Agent (Risk 1)

**What breaks**: If `begin_write()`, any intermediate redb operation, or `commit()` fails:
- `Create` path: ZSet weight becomes 1 (record "present") but the record was never written to disk. `get_record_bytes` will see ZSet weight=1, attempt the redb read, find nothing, return `Ok(None)`. The phantom ZSet entry persists until restart. `table_len` returns an inflated count.
- `Delete` path: ZSet entry is removed (record "gone") but the record still exists on disk. `get_record_bytes` short-circuits on weight=0 and returns `Ok(None)` — **the record is permanently unreadable** for the lifetime of the process.

In `apply_batch`, all N ZSet mutations are applied inside the loop before `commit()` at line 221. A single `commit()` failure makes all N mutations permanent in memory while none persist to disk.

**Exact fix** — move all ZSet mutations to *after* `write_txn.commit()` succeeds:

```rust
// apply_mutation — correct ordering:
let write_txn = self.db.begin_write()?;
{
    let mut records = write_txn.open_table(RECORDS_TABLE)?;
    // ... all redb writes ...
}
write_txn.commit()?;   // ← commit first; if this fails, ZSet is untouched

// THEN update ZSet (guaranteed to match disk state):
let zset = self.zsets.entry(SmolStr::new(table)).or_default();
match op {
    Operation::Delete => { zset.remove(id); }
    _ => { zset.insert(SmolStr::new(id), 1); }
}
```

Apply the same reordering in `apply_batch`: collect all `(table, id, op)` tuples during the redb loop, then apply all ZSet mutations only after `commit()` succeeds.

---

### CRIT-2 — Table Name Colon Invariant: documented but never enforced

**File**: `src/db/db.rs:98–100` (`make_key`), `src/db/db.rs:83` (`rebuild_zsets_from_records`)
**Confirmed by**: DB Architecture Agent (Gap B), Systems Correctness Agent (Risk 2)

**What breaks**: `make_key("a:b", "c")` produces `"a:b:c"`. On restart, `split_once(':')` in `rebuild_zsets_from_records` extracts table=`"a"`, id=`"b:c"`. The record is silently attributed to the wrong table. `get_record_bytes("a:b", "c")` checks ZSet weight=0 (wrong table), returns `Ok(None)` — the record appears to not exist even though it is on disk. `SpookyDbError::InvalidKey` exists but is **never constructed anywhere in the codebase**.

**Exact fix**:

```rust
// Add a shared validator used by make_key, apply_mutation, apply_batch, ensure_table:
#[inline]
fn validate_table_name(table: &str) -> Result<(), SpookyDbError> {
    if table.contains(':') {
        Err(SpookyDbError::InvalidKey(format!(
            "table name must not contain ':': {:?}", table
        )))
    } else {
        Ok(())
    }
}

// In apply_mutation (line 118), apply_batch (line 172 loop), ensure_table (line 402):
validate_table_name(table)?;
```

Add test: `apply_mutation("a:b", ...)` returns `Err(SpookyDbError::InvalidKey(...))`.

---

### CRIT-3 — `DbBackend` Trait Not Object-Safe: `bulk_load` uses `impl IntoIterator`

**File**: `src/db/db.rs:443–446` (trait), `src/db/db.rs:482–488` (impl)
**Confirmed by**: DB Architecture Agent (Gap A)

**What breaks**: `DbBackend` is not object-safe. `context.md` (line 263) and the SSP migration plan show `Circuit.db: Box<dyn DbBackend>` as the integration target. This code will not compile. Every SSP engineer attempting Step 3 of the migration plan hits a compiler error before any actual integration work.

**Exact fix**:

```rust
// Before:
fn bulk_load(&mut self, records: impl IntoIterator<Item = BulkRecord>) -> Result<(), SpookyDbError>;

// After:
fn bulk_load(&mut self, records: Vec<BulkRecord>) -> Result<(), SpookyDbError>;
```

Update the concrete `SpookyDb::bulk_load` signature to match. No logic changes required.

Test: `let _: Box<dyn DbBackend> = Box::new(SpookyDb::new(path)?);` compiles without error.

---

### CRIT-4 — `DbBackend::get_record_bytes` Silently Swallows I/O Errors

**File**: `src/db/db.rs:421` (trait), `src/db/db.rs:457–459` (impl)
**Confirmed by**: DB Architecture Agent (Issue 1), Clean Code Agent (API Clarity 1), Systems Correctness Agent (Contract E)

**What breaks**: A redb `StorageError` (corrupt page, disk-full, mmap failure) at read time becomes a silent `None`. Any SSP join probe or filter predicate using `DbBackend::get_record_bytes` silently treats a storage error as a missing record — no panic, no log, no propagation. Wrong query results are produced with no error surfaced.

**Exact fix**:

```rust
// Trait (line 421) — change to:
fn get_record_bytes(&self, table: &str, id: &str) -> Result<Option<Vec<u8>>, SpookyDbError>;

// Impl (lines 457–459) — change to:
fn get_record_bytes(&self, table: &str, id: &str) -> Result<Option<Vec<u8>>, SpookyDbError> {
    self.get_record_bytes(table, id)  // delegates to inherent method, no .ok() swallowing
}
```

Update all call sites that currently pattern-match on `Option<Vec<u8>>` to propagate `?`.

---

### CRIT-5 — `set_i64_at` / `set_u64_at` Missing Stale FieldSlot Assertion

**File**: `src/spooky_record/write_op.rs:250–258`, `src/spooky_record/write_op.rs:261–270`
**Confirmed by**: Systems Correctness Agent (Silent Failure Path 5)

**What breaks**: After a structural mutation (`add_field`/`remove_field`), the record's `generation` increments. A stale `FieldSlot` from before the mutation has the old `generation`. `set_f64_at` (line 277) correctly has `debug_assert_eq!(slot.generation, self.generation, "stale FieldSlot")`. `set_i64_at` and `set_u64_at` **lack this assertion**. A stale slot writes to a wrong buffer offset silently in all builds — data corruption with no error.

**Exact fix** — add the same assertion as `set_f64_at`:

```rust
// set_i64_at (line 250):
pub fn set_i64_at(&mut self, slot: &FieldSlot, value: i64) -> Result<(), RecordError> {
    debug_assert_eq!(slot.generation, self.generation, "stale FieldSlot");
    if slot.type_tag != TAG_I64 { return Err(...); }
    // ... rest unchanged

// set_u64_at (line 261):
pub fn set_u64_at(&mut self, slot: &FieldSlot, value: u64) -> Result<(), RecordError> {
    debug_assert_eq!(slot.generation, self.generation, "stale FieldSlot");
    if slot.type_tag != TAG_U64 { return Err(...); }
    // ... rest unchanged
```

---

## Phase 1 — Correctness Hardening

Foundation correctness fixes. Each step must compile and pass all 125 tests before proceeding.

### 1.1 ZSet / Disk Atomicity → see CRIT-1

### 1.2 Table Name Colon Invariant → see CRIT-2

### 1.3 `from_bytes` Index Sort Order Validation

**File**: `src/serialization.rs:407–421`
**Confirmed by**: Systems Correctness Agent (Risk 4)

The binary search in `find_field` assumes the index is sorted by `name_hash`. `from_bytes` validates buffer length but not sort order. Hand-crafted buffers or any future write path that forgets to sort will silently corrupt all field lookups.

```rust
// Add to from_bytes, after the length check:
#[cfg(debug_assertions)]
{
    for i in 1..field_count {
        let h_prev = u64::from_le_bytes(
            buf[HEADER_SIZE + (i-1)*INDEX_ENTRY_SIZE..][..8].try_into().unwrap()
        );
        let h_curr = u64::from_le_bytes(
            buf[HEADER_SIZE + i*INDEX_ENTRY_SIZE..][..8].try_into().unwrap()
        );
        debug_assert!(h_prev <= h_curr,
            "index not sorted: hash[{}]={:#x} > hash[{}]={:#x}", i-1, h_prev, i, h_curr);
    }
}
```

Test: hand-construct a buffer with reversed hashes; assert `from_bytes` panics in debug.

### 1.4 `SpookyRecord::new` Field Count Header Validation

**File**: `src/spooky_record/record.rs:15–21`
**Confirmed by**: Systems Correctness Agent (Risk 3)

`SpookyRecord::new` trusts the caller-provided `field_count` without checking the buffer header. A wrong count causes silent wrong field lookups.

```rust
pub fn new(data_buf: &'a [u8], field_count: usize) -> Self {
    debug_assert!(
        data_buf.len() >= 4 &&
        u32::from_le_bytes(data_buf[0..4].try_into().unwrap()) as usize == field_count,
        "SpookyRecord::new: field_count {} doesn't match header",
        field_count,
    );
    Self { data_buf, field_count }
}
```

### 1.5 Spurious `-1` Delta on Non-Existent Delete

**File**: `src/db/db.rs:206–210` (`apply_batch`), `src/db/db.rs:149` (`apply_mutation`)
**Confirmed by**: Systems Correctness Agent (Risk 8)

`Operation::Delete.weight()` = -1 always, even if the record was never present. `BatchMutationResult.membership_deltas` will contain `{id: -1}` for a delete of a nonexistent record. Any downstream consumer applying this delta to their own ZSet will corrupt it (weight goes from 0 to -1).

**Fix in `apply_batch`**: only emit the delta if the ZSet actually contained the record:

```rust
let had_record = zset.contains_key(id.as_str());
if matches!(op, Operation::Delete) {
    zset.remove(id.as_str());
}
let weight = if matches!(op, Operation::Delete) && !had_record { 0 } else { op.weight() };
if weight != 0 {
    membership_deltas.entry(table.clone()).or_default().insert(id.clone(), weight);
}
```

Test: delete a key never created, assert `membership_deltas` is empty.

### 1.6 `apply_zset_delta_memory` Weight Invariant

**File**: `src/db/db.rs:364–370`
**Confirmed by**: Systems Correctness Agent (Risk 6)

`apply_zset_delta_memory` accepts any delta value. A miscalculated delta can produce weight=2 or weight=-1 in the ZSet, breaking the membership invariant (weight ∈ {0, 1}) and causing the ZSet guard to behave incorrectly.

```rust
// After *entry += weight:
debug_assert!(
    *entry >= 0 && *entry <= 1,
    "ZSet weight out of range after delta: table={:?}, id={:?}, weight={}",
    table, id, *entry
);
```

---

## Phase 2 — Architecture Cleanup

Fix after Phase 1 is stable. Each step is independently buildable.

### 2.1 `DbBackend` Object Safety → see CRIT-3

### 2.2 `get_record_bytes` Error Propagation → see CRIT-4

### 2.3 Fix `ensure_table` / `table_exists` Semantic Mismatch

**File**: `src/db/db.rs:379–404`
**Confirmed by**: DB Architecture Agent (Issue 3), Clean Code Agent (Doc Error 1, Structural 2)

`ensure_table` creates an empty ZSet entry. `table_exists` checks `!z.is_empty()`. So a table that was "ensured" does not "exist" until it has a record — the opposite of what both the doc comment and `db_architecture.md` state. The `db_architecture.md` (line 405–406) documents `contains_key` as the intended check.

**Fix**: change `table_exists` to use `contains_key`:

```rust
// Before (line 379–384):
pub fn table_exists(&self, table: &str) -> bool {
    self.zsets.get(table).map(|z| !z.is_empty()).unwrap_or(false)
}

// After:
pub fn table_exists(&self, table: &str) -> bool {
    self.zsets.contains_key(table)
}
```

Also update `DbBackend` trait method, `README.md:277` ("has been registered"), and the test assertion comment at line 699–701.

### 2.4 `apply_mutation` Return Key Format: bare `id` not flat key

**File**: `src/db/db.rs:149`
**Confirmed by**: DB Architecture Agent (Issue 6)

`apply_mutation` returns `(SmolStr::new(&key), i64)` where `key` is the full flat `"table:id"` string. `BatchMutationResult.membership_deltas` uses bare `id` as the ZSet key. Any caller that aggregates single-mutation results to manually build a `BatchMutationResult`-equivalent will produce a ZSet keyed by `"table:id"` strings — incompatible with `apply_batch` output.

```rust
// Before (line 149):
Ok((SmolStr::new(&key), weight))

// After:
Ok((SmolStr::new(id), weight))
```

Update doc comment on `apply_mutation` to state the returned `SmolStr` is the bare record id.

### 2.5 Gate `apply_zset_delta_memory` as `pub(crate)`

**File**: `src/db/db.rs:362`
**Confirmed by**: DB Architecture Agent (Issue 4), Clean Code Agent (API Clarity 3)

`apply_zset_delta_memory` is `pub` but has no external consumer and no checkpoint system to wire it to. Any SSP code can call it and cause ZSet/disk divergence.

```rust
// Before:
pub fn apply_zset_delta_memory(&mut self, table: &str, delta: &ZSet) {

// After:
pub(crate) fn apply_zset_delta_memory(&mut self, table: &str, delta: &ZSet) {
// TODO: expose as pub when checkpoint/recovery system is implemented.
```

### 2.6 Add `get_record_typed` to `DbBackend` Trait

**File**: `src/db/db.rs:415–450`
**Confirmed by**: DB Architecture Agent (Issue 5)

`get_record_typed` is only available on concrete `SpookyDb`, not through `dyn DbBackend`. SSP code using the trait pattern cannot call it — but `context.md` read path examples reference it heavily. SSP engineers will attempt to call it through the trait and fail.

```rust
// Add to DbBackend trait:
fn get_record_typed(
    &self,
    table: &str,
    id: &str,
    fields: &[&str],
) -> Result<Option<SpookyValue>, SpookyDbError>;
```

The `SpookyDb` concrete implementation already exists at line 301–322.

### 2.7 VERSION_TABLE Staleness Contract

**File**: `src/db/db.rs:138–145`
**Confirmed by**: DB Architecture Agent (Issue 8), Performance Agent (item 6), Systems Correctness Agent (Risk 7)

An `Update` with `version: None` leaves the `VERSION_TABLE` entry at the previous version, which no longer corresponds to the current data. There is no code-level enforcement that callers must provide a version on every write. Document this explicitly on `apply_mutation` itself (not just on `DbMutation.version`):

```rust
/// **Version contract**: if `version` is `None`, the VERSION_TABLE entry is
/// left unchanged. An `Update` with `version: None` leaves the previous
/// version in place — it may no longer reflect the current data.
/// Callers using version numbers for conflict detection MUST pass the
/// new version on every write, including Updates.
```

Also: add `version: Option<u64>` to `BulkRecord` and write it to `VERSION_TABLE` in `bulk_load` so that version tracking is consistent across all ingestion paths:

```rust
pub struct BulkRecord {
    pub table: SmolStr,
    pub id: SmolStr,
    pub data: Vec<u8>,
    pub version: Option<u64>,   // add this
}
// In bulk_load, inside the loop, open VERSION_TABLE and write if Some:
if let Some(ver) = record.version {
    ver_table.insert(key.as_str(), ver)?;
}
```

### 2.8 CLAUDE.md Stale References Cleanup

**File**: `CLAUDE.md`, `MEMORY.md`
**Confirmed by**: DB Architecture Agent (Issue 7), Clean Code Agent (Dead Code Inventory)

`db_current.rs` no longer exists on disk. `CLAUDE.md` still documents it as the "Old `SpookyDb` (partially broken, do not extend)" in a lengthy section that actively misleads any new contributor or AI agent. `MEMORY.md` also references it. Update both files to reflect the current architecture: `db.rs` + `types.rs` are the active, tested implementation.

Also remove consumer-project internal references from doc comments (lines 32, 233, 410 in `db.rs`): replace `"circuit.rs"` / `"circuit runner"` with generic descriptions.

### 2.9 UFCS in `impl DbBackend for SpookyDb` to Eliminate Apparent Recursion

**File**: `src/db/db.rs:452–455`
**Confirmed by**: Clean Code Agent (Naming 2)

`self.get_table_zset(table)` inside `impl DbBackend for SpookyDb` resolves correctly to the inherent method under NLL, but reads as infinite recursion. Other delegations in the same block use UFCS correctly. Fix for consistency and readability:

```rust
// Before:
fn get_table_zset(&self, table: &str) -> Option<&ZSet> {
    self.get_table_zset(table)
}

// After:
fn get_table_zset(&self, table: &str) -> Option<&ZSet> {
    SpookyDb::get_table_zset(self, table)
}
```

Apply the same fix to `ensure_table` and `get_zset_weight` delegations.

---

## Phase 3 — Performance

Run after Phase 1 and 2 stabilise. **Always add the benchmark first, then optimize.** The entire persistence layer currently has zero Criterion/divan coverage.

### 3.1 `make_key` Stack Allocation — top allocation on every I/O call

**File**: `src/db/db.rs:97–100`
**Confirmed by**: Performance Agent (Rank 1), Clean Code Agent (Naming 1)

`format!("{}:{}", table, id)` allocates a `String` on EVERY call — `get_record_bytes`, `get_version`, `apply_mutation`, `apply_batch` (per mutation), `bulk_load` (per record). At 10k ops/sec this is 10k+ `malloc`+`free` cycles/sec.

**Benchmark first**: add `bench_db_layer/make_key/short` and `bench_db_layer/make_key/uuid` to `spooky_bench.rs`.

```rust
// After benchmarking, replace with:
fn make_key(table: &str, id: &str) -> arrayvec::ArrayString<512> {
    let mut s = arrayvec::ArrayString::<512>::new();
    s.push_str(table);
    s.push(':');
    s.push_str(id);
    s
    // All callers use .as_str() — no callsite changes needed.
}
```

`arrayvec` is already a project dependency.

### 3.2 `get_version` Missing ZSet Guard

**File**: `src/db/db.rs:327–334`
**Confirmed by**: Performance Agent (item 2)

`get_version` opens a redb read transaction even for keys with ZSet weight=0 (known absent). This is 100-500ns of wasted I/O for every absent-key version check.

**Benchmark first**: add `bench_db_layer/get_version/miss`.

```rust
pub fn get_version(&self, table: &str, id: &str) -> Result<Option<u64>, SpookyDbError> {
    if self.get_zset_weight(table, id) == 0 {
        return Ok(None);  // ← add this guard
    }
    let key = make_key(table, id);
    let read_txn = self.db.begin_read()?;
    ...
}
```

### 3.3 `apply_batch` Table-Name Clone Reduction

**File**: `src/db/db.rs:185`
**Confirmed by**: Performance Agent (Rank 2)

`self.zsets.entry(table.clone()).or_default()` clones `table: SmolStr` on every loop iteration. For a batch of 1000 mutations to the same table, this is 1000 clones. SmolStr inlines ≤22 bytes (memcpy), but the hash lookup is repeated unnecessarily.

**Benchmark first**: add `bench_db_layer/apply_batch/1000` (same table).

**Fix**: sort the incoming mutations slice by table name before the loop, then cache the ZSet pointer while table is unchanged. Sorting also benefits redb (sequential B-tree writes are faster than random):

```rust
mutations.sort_unstable_by(|a, b| a.table.cmp(&b.table));
let mut last_table: &str = "";
let mut zset: Option<&mut ZSet> = None;
for mutation in &mutations {
    if mutation.table.as_str() != last_table {
        last_table = &mutation.table;
        zset = Some(self.zsets.entry(mutation.table.clone()).or_default());
    }
    // use zset.as_mut().unwrap()
}
```

### 3.4 `changed_tables_set` Eliminate Double-Collect

**File**: `src/db/db.rs:163–165`, `src/db/db.rs:226`
**Confirmed by**: Performance Agent (Rank 3)

`FastHashSet<SmolStr>` is allocated, populated, then `.into_iter().collect()`'d into `Vec<SmolStr>`. One extra allocation and full copy per batch.

**Fix** (no benchmark needed — straightforward regression-free):

```rust
// Replace:
let mut changed_tables_set: FastHashSet<SmolStr> = FastHashSet::default();
// loop: changed_tables_set.insert(table.clone());
changed_tables: changed_tables_set.into_iter().collect(),

// With:
let mut changed_tables: Vec<SmolStr> = Vec::new();
// loop: if !changed_tables.contains(&table) { changed_tables.push(table.clone()); }
changed_tables,
```

For realistic table counts (< 100 distinct tables per batch), linear `Vec::contains` beats FxHasher overhead.

### 3.5 `apply_batch` ZSet/disk Inconsistency Detection (Bonus Correctness)

After CRIT-1 fix (ZSet after commit), add a ZSet/disk consistency check to `get_record_bytes` for the case where ZSet says present but redb returns None:

```rust
// After ZSet guard passes (weight > 0):
match tbl.get(key.as_str())? {
    Some(guard) => Ok(Some(guard.value().to_vec())),
    None => Err(SpookyDbError::InvalidKey(format!(
        "ZSet/disk inconsistency: table={:?} id={:?} in ZSet but not on disk", table, id
    ))),
}
```

### 3.6 `set_field` Scratch Buffer Reuse

**File**: `src/spooky_record/write_op.rs:208`
**Confirmed by**: Performance Agent (Rank 6)

Every `set_field` call allocates `Vec::new()` for field bytes. Extract an internal helper accepting `&mut Vec<u8>` scratch:

```rust
pub fn set_field<V: RecordSerialize>(&mut self, name: &str, value: &V) -> Result<(), RecordError> {
    let mut scratch = Vec::new();
    self.set_field_inner(name, value, &mut scratch)
}
fn set_field_inner<V: RecordSerialize>(&mut self, name: &str, value: &V, scratch: &mut Vec<u8>) -> Result<(), RecordError> {
    scratch.clear();
    // ... existing logic using scratch instead of local Vec
}
```

**Benchmark first**: `set_values` group in `spooky_bench.rs` — isolate allocation cost vs copy cost.

### 3.7 `migration_op.rs` Stack-Allocate Index Entry Buffer

**File**: `src/spooky_record/migration_op.rs:145–149`
**Confirmed by**: Performance Agent (Rank 7)

`read_all_index_entries` allocates `Vec::with_capacity(n)` for up to 32 `IndexEntry` structs. Use `ArrayVec<IndexEntry, 32>` (already a dependency) to keep it on the stack:

```rust
// Before:
fn read_all_index_entries(&self, n: usize) -> Result<Vec<IndexEntry>, RecordError> {
    let mut entries = Vec::with_capacity(n);

// After:
fn read_all_index_entries(&self, n: usize) -> Result<arrayvec::ArrayVec<IndexEntry, 32>, RecordError> {
    let mut entries = arrayvec::ArrayVec::new();
```

### 3.8 Benchmark Strategy

**Add to `benches/spooky_bench.rs`** in this order (benchmark before optimizing each):

```rust
fn bench_db_layer(c: &mut Criterion) {
    let mut group = c.benchmark_group("db_layer");

    // 1. make_key — isolated micro
    group.bench_function("make_key/short",  |b| b.iter(|| make_key(black_box("users"),  black_box("alice"))));
    group.bench_function("make_key/uuid",   |b| b.iter(|| make_key(black_box("events"), black_box("550e8400-e29b-41d4-a716-446655440000"))));

    // 2. apply_batch at different sizes (same table)
    for n in [1, 10, 100, 1000, 10_000] {
        group.bench_function(format!("apply_batch/{n}"), |b| {
            b.iter_batched(|| build_batch(n), |m| db.apply_batch(black_box(m)).unwrap(), SmallInput)
        });
    }

    // 3. get_record_bytes: ZSet hit vs miss
    group.bench_function("get_record_bytes/hit",  |b| b.iter(|| db.get_record_bytes("users", "u0")));
    group.bench_function("get_record_bytes/miss", |b| b.iter(|| db.get_record_bytes("users", "nonexistent")));

    // 4. get_version: miss path (no ZSet guard currently)
    group.bench_function("get_version/miss", |b| b.iter(|| db.get_version("users", "nonexistent")));

    // 5. rebuild_zsets at scale
    // (requires a pre-populated db — run as a separate bench binary or setup fn)

    // 6. find_field cutoff validation: N=2,4,5,8,12,16,32
    for n in [2, 4, 5, 8, 12, 16, 32] {
        group.bench_function(format!("find_field/n{n}"), |b| {
            let rec = make_record_with_n_fields(n);
            b.iter(|| rec.find_field(black_box("target_field")))
        });
    }

    group.finish();
}
```

**Profile workflow**:
```bash
# Save baseline before any optimization:
cargo bench --bench spooky_bench -- --save-baseline before_opt

# macOS: profile apply_batch with Instruments:
cargo install cargo-instruments
cargo instruments -t Alloc --bench spooky_bench -- db_layer/apply_batch/1000

# Compare after optimization:
cargo bench --bench spooky_bench -- --baseline before_opt
open target/criterion/report/index.html
```

---

## Phase 4 — Clean Code (parallelisable with Phase 3)

### 4.1 Fix Documentation Errors

**4.1a — `ensure_table` doc claims `table_exists()` returns `true`** (wrong):

**File**: `src/db/db.rs:398–401`

```rust
// Replace the doc comment with:
/// Pre-registers a table by creating an empty ZSet entry.
///
/// After this call, `table_names()` will list the table and `table_exists()`
/// will return `true`. The table name must not contain ':'.
pub fn ensure_table(&mut self, table: &str) {
```

(Note: this doc becomes correct once Phase 2.3 changes `table_exists` to use `contains_key`.)

**4.1b — `IndexEntry` / `FieldSlot` doc collision in `types.rs`**:

**File**: `src/types.rs:35–66`

The doc block before `IndexEntry` accidentally contains the `FieldSlot` description as its last sentence. Split into two separate doc blocks — one for `IndexEntry`, one for `FieldSlot`.

**4.1c — `error.rs` typos and informal casing**:

**File**: `src/error.rs:18–19`

```rust
// Before:
RecordError::SerializationNotObject => write!(f, "Can't Serialize none Object Types"),
RecordError::TooManyFields => write!(f, ">= 32 Entetys"),

// After:
RecordError::SerializationNotObject => write!(f, "cannot serialize non-object types"),
RecordError::TooManyFields => write!(f, "record exceeds the 32-field limit"),
```

**4.1d — README.md `table_exists` description**:

**File**: `README.md:277` — update "Check if a table has been registered" to "Check if a table exists (has at least one record or was pre-registered with `ensure_table`)".

### 4.2 Document `BENCH_CBOR` at Declaration Site

**File**: `src/db/db.rs:503–523`

```rust
/// CBOR-encoded test record used by all db-layer tests.
/// Encodes a 12-field user record: { id (str), name (str), age (i64),
/// count (u64), score (f64), active (bool), deleted (bool), metadata (null),
/// tags (array), profile (object), history (array), mixed_array (array) }.
const BENCH_CBOR: &[u8] = &[
```

### 4.3 `SpookyDbError` → `thiserror` (optional quality of life)

**File**: `src/db/types.rs:12–72`, `Cargo.toml`

`thiserror` is not currently a dependency. Adding it reduces the 60-line manual `Display` + `From` block to ~10 lines. The six individual `From<redb::XxxError>` impls (each with `.into()` chain) become explicit and documented. This is a low-risk, pure cosmetic refactor:

```toml
# Cargo.toml:
thiserror = "1"
```

```rust
#[derive(Debug, thiserror::Error)]
pub enum SpookyDbError {
    #[error("redb error: {0}")]
    Redb(#[from] redb::Error),
    #[error("serialization error: {0}")]
    Serialization(String),
    #[error("invalid key: {0}")]
    InvalidKey(String),
}
// Individual From<redb::DatabaseError> etc. impls remain to avoid breaking changes.
```

### 4.4 Naming Fixes

- **`make_key` → `format_key`** (`db.rs:97`): signals it allocates via `format!`. More precise than `make_key`. All callers are internal.
- **`Operation::weight(self)` → `weight(&self)`** (`types.rs:101`): idiomatic for `Copy` types. No semantic change.
- **`type TableName = SmolStr`** in `types.rs`: distinguishes table-name SmolStr from record-id SmolStr in `BatchMutationResult` fields. Update `membership_deltas: FastMap<TableName, ZSet>`, `content_updates: FastMap<TableName, FastHashSet<RowKey>>`, `changed_tables: Vec<TableName>`.

### 4.5 Remove Dead Text

- **`src/lib.rs:2–3`**: delete two commented-out `//pub mod spooky_record;` and `//pub mod spooky_record_mut;` lines.
- **`src/spooky_record/record_mut.rs:86–104`**: either implement the three TODO'd methods (`into_bytes`, `as_bytes`) as one-liners delegating to `pub data_buf`, or replace the block comment with a note that `data_buf` is the intentional public accessor.
- **`src/types.rs:59` `#[allow(dead_code)]` on `FieldSlot`**: verify with `cargo check`; if the field is actively used, remove the attribute.

### 4.6 `write_field_into` Unknown Type Should Error

**File**: `src/serialization.rs:288–296`
**Confirmed by**: Systems Correctness Agent (Silent Failure Path 1)

The fallthrough branch silently writes `TAG_NULL` for unknown value types. Change to an error:

```rust
// Before:
} else {
    TAG_NULL  // unknown type → silently null
}

// After:
} else {
    return Err(RecordError::SerializationNotObject);
}
```

### 4.7 Remove Consumer-Project References from Library Doc Comments

**File**: `src/db/db.rs:32`, `src/db/db.rs:233`, `src/db/db.rs:410`

Replace `"circuit runner"`, `"circuit.rs"`, and `"init_load in circuit.rs"` with generic descriptions. Library doc comments must not reference files in consumer projects.

---

## Cross-Agent Confirmed Issues

Issues flagged by **2 or more agents** — highest confidence, fix first within each phase.

| Issue | Agents | Phase | Priority |
|---|---|---|---|
| ZSet mutated before redb commit, no rollback | Correctness + Architecture | CRIT-1 | Highest |
| Table name `':'` never validated, `InvalidKey` never used | Correctness + Architecture | CRIT-2 | Highest |
| `DbBackend::get_record_bytes` silently swallows I/O errors | Architecture + Clean Code + Correctness | CRIT-4 | Highest |
| `DbBackend` not object-safe (`bulk_load` with `impl IntoIterator`) | Architecture (SSP blocker) | CRIT-3 | Highest |
| `ensure_table` / `table_exists` semantic mismatch | Architecture + Clean Code | Phase 2.3 | High |
| `apply_zset_delta_memory` should be `pub(crate)` | Architecture + Clean Code | Phase 2.5 | Medium |
| `make_key` heap allocation on every I/O call | Performance + Clean Code | Phase 3.1 | High |
| `bulk_load` does not write to VERSION_TABLE | Performance + Correctness | Phase 2.7 / 3.5 | Medium |
| Stale CLAUDE.md / MEMORY.md referencing deleted `db_current.rs` | Architecture + Clean Code | Phase 2.8 | Low |
| `apply_mutation` returns full flat key instead of bare id | Architecture + Correctness (delta format) | Phase 2.4 | High |

---

## Skills Used

| Skill | Key Findings |
|---|---|
| `spooky-arch-reviewer` | `DbBackend` object-safety violation; `get_record_bytes` error swallowing; `apply_mutation` return key format; `ensure_table`/`table_exists` naming; `apply_zset_delta_memory` visibility; VERSION_TABLE staleness contract |
| `clean-architecture` | Layer boundary analysis; SSP integration friction map; 9-step recommended refactoring order; integration blocker ranking |
| `m10-performance` | Allocation audit table (10 ranked hot paths); `apply_batch` table-clone caching; `changed_tables_set` double-collect; benchmark coverage gaps; profiling workflow |
| `rust-performance` | `make_key` ArrayString fix; `get_version` ZSet guard; `find_field` cutoff benchmark; `from_bytes` validation cost analysis |
| `rust-optimise` | `set_field` scratch buffer reuse; `migration_op.rs` ArrayVec; `from_cbor` intermediate BTreeMap elimination path |
| `clean-code` | `BENCH_CBOR` documentation; `ensure_table` doc error; `IndexEntry`/`FieldSlot` doc collision; error.rs typos; consumer-project doc references; commented-out dead `pub mod` lines |
| `rust-refactor` | `DbBackend` trait API design; `Operation::weight(&self)` idiomatic fix; `TableName` type alias; UFCS in impl block |
| `systems-programming-rust-project` | ZSet/disk atomicity analysis; sort-order invariant in binary format; `SpookyRecord::new` field_count validation; stale FieldSlot assertion gaps; spurious delete delta; silent failure path audit |

---

*Total issues found: **5 Critical** · **9 Architecture** · **8 Performance** · **12 Clean Code** · **10 Correctness invariants***
